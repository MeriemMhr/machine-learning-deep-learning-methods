{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Introduction to Multi-task Learning for House Price and Category Prediction (Cont'd) | Part 2\n",
    "\n",
    "---\n",
    "\n",
    "Here is the next phase of our project.\n",
    "\n",
    "In this Jupyter notebook, we will be carrying out the advanced steps of our machine learning journey. As we progress, we'll delve into building a multi-task model using PyTorch Lightning, where we'll develop a feed-forward neural network with both shared and task-specific layers tailored to handle the dual objectives of predicting house prices and categorizing house types.\n",
    "\n",
    "We will explore and experiment with various activation functions and optimizers to determine the optimal combinations for our model. Additionally, we'll design and integrate appropriate loss functions for both the regression and classification tasks into a cohesive training objective. To ensure the robustness and accuracy of our model, we will rigorously evaluate its performance using suitable metrics.\n",
    "\n",
    "Furthermore, we'll leverage advanced PyTorch Lightning features, such as logging, callbacks, and the Trainer API, to enhance our model training and evaluation processes. Hyperparameter tuning will also be a key focus, utilizing Optuna to refine and optimize our model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this initial section of our notebook, we focus on setting the stage for the predictive modeling process. We begin by ensuring our Python environment is properly configured to access the project's directory structure. This allows us to seamlessly load our raw and processed data for examination and further manipulation. We load the datasets using custom functions, presumably designed to handle specific preprocessing tasks, thus preparing our datasets for analysis. Following this, we split our data into training and testing sets, which is a crucial step in evaluating the performance of our model later on. With our data split, we proceed to initialize our model training process. We import the `train` function, which will ingest our training data to start the model's learning phase. Finally, we set up model evaluation, readying a function to assess the performance of our trained model against the test set. Each step is meticulously designed to ensure a smooth transition from raw data to a ready-to-evaluate predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the path to the root of the project\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"../data/raw/train.csv\")\n",
    "df_clean = load_data(\"../data/processed/processed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_clean, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.train_model import train\n",
    "\n",
    "model = train(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_model import evaluate_model\n",
    "\n",
    "results = evaluate_model(model, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Networks with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we delve into the intricacies of enhancing our predictive model by infusing it with the power of embeddings. The notebook guides us through the innovative process of transforming categorical data into rich, numerical embeddings, which capture the essence of the categories in a format that our neural network can interpret and learn from. These embeddings are then incorporated into a regression network built using PyTorch, a framework renowned for its flexibility and performance in building complex models. We meticulously set up the regression network, ensuring that it can effectively utilize the embedded information to make precise predictions. This step is fundamental as it bridges the gap between raw categorical data and a machine learning model that can process this data, thus laying down a strong foundation for our modeling pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv',usecols=[\"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n",
    "                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=[[col,len(df[col].unique())] for col in df.columns],columns=['Feature','Unique Items']).style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YearsSinceBuilt'] = datetime.datetime.now().year - df.YearBuilt\n",
    "df.drop('YearBuilt',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we embark on the task of architecting the foundational structure of our deep learning model. Here, we meticulously design a neural network that will learn to predict house prices and categorize houses. This involves establishing a multi-layered architecture where each layer is engineered to process and pass on information in a form that is beneficial for the subsequent layers, culminating in accurate predictions. We carefully choose activation functions that introduce non-linearity, enabling the network to handle complex patterns within the data. By leveraging the power of PyTorch Lightning, we streamline the construction process, ensuring our model is not only powerful and sophisticated but also efficient and scalable. This neural network will serve as the cornerstone of our predictive analysis, and its robust design is pivotal to the success of our multi-task learning objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_cont, out_sz, layers, drop=0.5):\n",
    "        super().__init__()\n",
    "        self.embed_repr = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dims])\n",
    "        self.embed_dropout = nn.Dropout(drop)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((val[1] for val in embedding_dim))\n",
    "        n_in = n_cont + n_emb\n",
    "        \n",
    "        for layer in layers:\n",
    "            layerlist.append(nn.Linear(n_in,layer))\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(layer))\n",
    "            layerlist.append(nn.Dropout(drop))\n",
    "            n_in = layer\n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "        \n",
    "    def forward(self, cat,cont):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embed_repr):\n",
    "            embeddings.append(e(cat[:,i]))\n",
    "        x = torch.cat(embeddings,1)\n",
    "        x = self.embed_dropout(x)\n",
    "        x_cont = self.bn_cont(cont)\n",
    "        x = torch.cat([x,x_cont],1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "model = Model(embedding_dims, len(cont_features), 1, [100,50], drop = .4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(df)\n",
    "test_size = int(batch_size*.15)\n",
    "train_cat = catTensor[:batch_size-test_size]\n",
    "test_cat = catTensor[batch_size-test_size:batch_size]\n",
    "train_cont = contTensor[:batch_size-test_size]\n",
    "test_cont = contTensor[batch_size-test_size:batch_size]\n",
    "y_train = y[:batch_size-test_size]\n",
    "y_test = y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model.forward(train_cat,train_cont)\n",
    "    loss = torch.sqrt(loss_function(y_pred,y_train))\n",
    "    losses.append(loss)\n",
    "    if i%50 == 0:\n",
    "        print(f\"Epoch {i} : {loss}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('RMSE Loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred=model(test_cat,test_cont)\n",
    "    loss=torch.sqrt(loss_function(y_pred,y_test))\n",
    "print('RMSE: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Multi-Task Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sophisticated approach to constructing a neural network, we are implementing a multi-task architecture within the PyTorch Lightning framework. Our MultiTaskModel class inherits from pl.LightningModule, allowing us to take full advantage of Lightning's organized and scalable approach to deep learning. The model's backbone is formed by shared layers that perform feature extraction, processed by different activation functions like ReLU and GELU to provide a variety of activation patterns.\n",
    "\n",
    "We incorporate an enhanced attention mechanism with a multi-head attention layer and a transformer encoder, signifying a state-of-the-art approach to sequence processing. For each task—regression and classification—we design separate heads. The regression head is meticulously structured with layers and dropout to refine the output continuously until we get the predicted price. Meanwhile, the classification head expands and contracts through its layers, fine-tuning the features before categorizing the house into one of the classes.\n",
    "\n",
    "In the training and testing steps, we compute losses tailored to each task: RMSE for regression and CrossEntropy for classification, highlighting the network's multi-tasking capability. The configure_optimizers method sets up an Adam optimizer with a cosine annealing learning rate scheduler, ensuring smooth and adaptive learning rate adjustments throughout training. This model is an intricate blend of modern neural network components, structured to extract and interpret complex patterns from the data, paving the way for accurate predictions and classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class MultiTaskModel(pl.LightningModule):\n",
    "    def __init__(self, num_features, num_classes, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Define shared layers for feature extraction\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Define attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=256, num_heads=8, dropout=0.2)\n",
    "        self.attention_linear = nn.Linear(256, 256)\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model=256, nhead=8, dim_feedforward=1024, dropout=0.2, activation='gelu'\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=6)\n",
    "        \n",
    "        # Define regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Define classification head\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(negative_slope=0.02),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through shared layers\n",
    "        shared_output = self.shared_layers(x)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        shared_output = shared_output.unsqueeze(0)\n",
    "        attention_output, _ = self.attention(shared_output, shared_output, shared_output)\n",
    "        attention_output = self.transformer_encoder(attention_output).squeeze(0)\n",
    "        attention_processed = self.attention_linear(attention_output)\n",
    "        \n",
    "        # Separate outputs for regression and classification\n",
    "        price = self.regression_head(attention_processed)\n",
    "        category = self.classification_head(attention_processed)\n",
    "        \n",
    "        return price, category\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Compute losses and log them\n",
    "        x, y_price, y_category = batch\n",
    "        price_pred, category_pred = self(x)\n",
    "        loss_price = torch.sqrt(self.mse_loss(price_pred.squeeze(), y_price))\n",
    "        loss_category = self.cross_entropy_loss(category_pred, y_category)\n",
    "        total_loss = loss_price + loss_category\n",
    "        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return total_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Evaluate the model on the test set\n",
    "        x, y_price, y_category = batch\n",
    "        price_pred, category_pred = self(x)\n",
    "        loss_price = torch.sqrt(self.mse_loss(price_pred.squeeze(), y_price))\n",
    "        loss_category = self.cross_entropy_loss(category_pred, y_category)\n",
    "        self.log_dict({\n",
    "            'test_rmse': loss_price,\n",
    "            'test_loss': loss_category\n",
    "        }, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss_price, loss_category\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Set up optimizers and schedulers\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.Cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation of the Multi-Task Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are fine-tuning the Multi-Task Neural Network to ensure optimal learning and generalization. Our MultiTaskModel is structured to undertake the dual challenge of predicting continuous values for house pricing and classifying house categories. By utilizing PyTorch Lightning's elegant interface, we streamline the training process with advanced techniques such as weighted loss functions to balance the classes and mean squared error loss to gauge the regression performance accurately.\n",
    "\n",
    "During training, we implement a mixed loss strategy to jointly optimize the regression and classification heads of our model. This ensures that the network does not favor one task over the other, fostering a balance that is critical in multi-task learning. Furthermore, the test step is carefully crafted to provide a detailed evaluation of the model's performance on unseen data, offering insights into its predictive accuracy and classification efficacy.\n",
    "\n",
    "The optimization setup concludes with an AdamW optimizer, known for its weight decay regularization, paired with a cosine annealing learning rate scheduler to adaptively fine-tune the learning rate throughout the training epochs. This nuanced approach to model training and evaluation is aimed at pushing the boundaries of our network's capacity to learn from complex datasets and perform multiple tasks simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizers and learning rate schedulers for training\n",
    "optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.001)\n",
    "return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
    "\n",
    "# Mean Squared Error Loss for regression\n",
    "self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# CrossEntropyLoss for classification, with class weights for balancing\n",
    "self.cross_entropy_loss = nn.CrossEntropyLoss(weight=self.hparams.class_weights)\n",
    "\n",
    "# Method to perform a training step\n",
    "def training_step(self, batch, batch_idx):\n",
    "    x, y_price, y_category = batch\n",
    "    price_pred, category_pred = self.forward(x)\n",
    "    loss_price = torch.sqrt(self.mse_loss(price_pred.squeeze(), y_price) / y_price.size(0))\n",
    "    loss_category = self.cross_entropy_loss(category_pred, y_category)\n",
    "    total_loss = loss_price + loss_category\n",
    "    self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    return total_loss\n",
    "\n",
    "# Method to perform a test step\n",
    "def test_step(self, batch, batch_idx):\n",
    "    x, y_price, y_category = batch\n",
    "    price_pred, category_pred = self.forward(x)\n",
    "    loss_price = torch.sqrt(self.mse_loss(price_pred.squeeze(), y_price) / y_price.size(0))\n",
    "    loss_category = self.cross_entropy_loss(category_pred, y_category)\n",
    "    self.log('test_rmse', loss_price, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    self.log('test_loss', loss_category, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    return {'test_rmse': loss_price, 'test_accuracy': loss_category}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with Optuna for the Multi-Task Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization is a crucial step in enhancing the performance of our Multi-Task Neural Network, and this is where Optuna comes into play. Optuna is a hyperparameter optimization framework that automates the process of finding the most effective hyperparameters for our model. In this script, we define an optimization objective function that trials different combinations of learning rates, dropout rates, the number of encoder layers, and attention heads.\n",
    "\n",
    "We instantiate our MultiTaskModel with the suggested parameters from Optuna and prepare our data, ready to be ingested by the model through DataLoader. With the integration of the PyTorchLightningPruningCallback, we enable the pruning feature that stops the training of unpromising trials, thereby saving time and computational resources. The pl.Trainer orchestrates the model's training process, adjusted for hyperparameter tuning, turning off unnecessary logging and progress tracking for efficiency.\n",
    "\n",
    "Upon completion, Optuna presents us with the study's best trials, allowing us to observe the most effective hyperparameters. The result is an optimized set of hyperparameters that we can use to train our Multi-Task Neural Network under the best conditions, striving for lower validation losses and improved generalization on unseen data. This approach not only enhances the performance but also contributes significantly to the understanding of how different hyperparameters impact our model's learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import pytorch_lightning as pl\n",
    "from models.multi_task_model import MultiTaskModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    num_encoder_layers = trial.suggest_int('num_encoder_layers', 1, 6)\n",
    "    num_heads = trial.suggest_categorical('num_heads', [4, 8, 16])\n",
    "    \n",
    "    # Instantiate the model with trial hyperparameters\n",
    "    model = MultiTaskModel(\n",
    "        learning_rate=learning_rate,\n",
    "        dropout_rate=dropout_rate,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_heads=num_heads\n",
    "    )\n",
    "    \n",
    "    # Prepare the dataset for the DataLoader\n",
    "    dataset = TensorDataset(torch.rand(100, 29), torch.rand(100, 1), torch.randint(0, 2, (100,)))\n",
    "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    \n",
    "    # Configure the trainer with PyTorch Lightning pruning callback\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=30,\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")],\n",
    "        logger=False,  # Disable logging for optimization runs\n",
    "        progress_bar_refresh_rate=0  # Disable progress bar for optimization runs\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, dataloader)\n",
    "    \n",
    "    # Objective to minimize: validation loss\n",
    "    val_loss = trainer.callback_metrics[\"val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters():\n",
    "    # Create an Optuna study which seeks to minimize the objective\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    # Optimize the study, running the objective function 50 times\n",
    "    study.optimize(objective, n_trials=50, timeout=3600)\n",
    "\n",
    "    # Output the results of the study\n",
    "    print(\"Study statistics: \")\n",
    "    print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "    print(\"  Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"    Value (val_loss): {best_trial.value}\")\n",
    "    print(\"    Params: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tune_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a comprehensive evaluation process to assess the performance of our neural network that has been trained to predict both house prices and categorize houses. This evaluation uses a specific set of test data processed through a tailored DataLoader setup to ensure the data format aligns perfectly with the model's requirements.\n",
    "\n",
    "We initiate the process by loading the trained model from a checkpoint, guaranteeing that we are evaluating the same parameters that were optimized during training. A DataLoader is then meticulously configured to feed the test data into the model, ensuring each feature and label is correctly formatted and batched. We leverage PyTorch Lightning's Trainer class, enhanced with custom logging and callback functions, to manage the testing process efficiently. This setup provides a streamlined and automated way to obtain vital metrics such as accuracy, loss, and other performance indicators, which are crucial for validating the model's real-world applicability and effectiveness. The results from this evaluation help in understanding the strengths and potential improvements for the multi-task learning model, guiding further refinements and deployment strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from models.multi_task_model import MultiTaskModel\n",
    "from preprocessing.data_preprocessor import load_data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils.logger import setup_logger, setup_callbacks\n",
    "\n",
    "def create_test_dataloader(df):\n",
    "    \"\"\"Prepare the test DataLoader from the DataFrame.\"\"\"\n",
    "    # Extract features and convert them to tensor\n",
    "    features = torch.tensor(df.drop(['SalePrice', 'HouseCategory'], axis=1).values, dtype=torch.float)\n",
    "    \n",
    "    # Convert SalePrice to tensor and reshape for consistency\n",
    "    prices = torch.tensor(df['SalePrice'].values, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Convert HouseCategory to tensor\n",
    "    categories = torch.tensor(df['HouseCategory'].values, dtype=torch.long)\n",
    "    \n",
    "    # Create TensorDataset and DataLoader for testing\n",
    "    dataset = TensorDataset(features, prices, categories)\n",
    "    return DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def evaluate_model(model_checkpoint, df_test):\n",
    "    \"\"\"Evaluate the model with a given checkpoint and test dataset.\"\"\"\n",
    "    # Load the model from checkpoint\n",
    "    model = MultiTaskModel.load_from_checkpoint(model_checkpoint)\n",
    "    \n",
    "    # Prepare test DataLoader\n",
    "    test_loader = create_test_dataloader(df_test)\n",
    "    \n",
    "    # Setup the PyTorch Lightning Trainer\n",
    "    trainer = Trainer(logger=setup_logger(), callbacks=setup_callbacks())\n",
    "    \n",
    "    # Test the model\n",
    "    results = trainer.test(model, dataloaders=test_loader)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load processed test data\n",
    "    df_test = load_data(\"../data/processed/processed_test.csv\")\n",
    "    \n",
    "    # Path to the model checkpoint\n",
    "    checkpoint_path = \"path_to_checkpoint.ckpt\"\n",
    "    \n",
    "    # Evaluate the model and print the results\n",
    "    evaluation_results = evaluate_model(checkpoint_path, df_test)\n",
    "    print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complementary - Enhanced Logging and Model Checkpointing Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this crucial component of our project setup, we establish advanced logging and checkpointing mechanisms using PyTorch Lightning's functionalities. The setup_logger function configures MLFlow to log experiments, enabling us to track all training metrics, parameters, and artifacts in a systematic and searchable database. This setup is invaluable for experiment management, allowing us to review past results and make data-driven decisions to refine our training process.\n",
    "\n",
    "Similarly, the setup_callbacks function ensures that our training process is resilient and efficient by setting up checkpoints. This is done through the ModelCheckpoint callback, which automatically saves the best performing models according to specified metrics, such as training loss. By saving only the top models, we conserve storage space and focus on the most promising model configurations. These mechanisms are instrumental in maintaining the integrity and continuity of our model training, facilitating both ease of experimentation and robustness in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "def setup_logger():\n",
    "    \"\"\"Sets up the MLFlow logger for tracking experiments.\n",
    "    \n",
    "    Returns:\n",
    "        MLFlowLogger: Configured MLFlow logger with experiment name and tracking URI.\n",
    "    \"\"\"\n",
    "    # Initialize MLFlow logger with specific experiment name and database URI\n",
    "    mlf_logger = MLFlowLogger(\n",
    "        experiment_name=\"lightning_logs\",\n",
    "        tracking_uri=\"sqlite:///../lightning_logs/mlruns.db\"\n",
    "    )\n",
    "    return mlf_logger\n",
    "\n",
    "def setup_callbacks():\n",
    "    \"\"\"Sets up PyTorch Lightning callbacks for model checkpointing.\n",
    "    \n",
    "    Returns:\n",
    "        list: List containing configured model checkpoint callback.\n",
    "    \"\"\"\n",
    "    # Configure model checkpointing to save the top 3 models based on validation loss\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='train_loss',\n",
    "        dirpath='../lightning_logs/checkpoints/',\n",
    "        filename='model-{epoch:02d}-{train_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "        auto_insert_metric_name=False\n",
    "    )\n",
    "    return [checkpoint_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Conclusion & Next Steps\n",
    "\n",
    "Throughout this project, we have meticulously executed several crucial steps in the development and refinement of a multi-task learning model capable of predicting house prices and categorizing houses. Starting from the initial data preparation, we moved on to embedding and feature engineering, model architecture design, and hyperparameter optimization. Each phase was designed to build upon the previous, ensuring a cohesive and systematic approach to tackling this complex machine learning problem.\n",
    "\n",
    "1. **Data Preparation**: We began by setting up our data infrastructure, ensuring all features were appropriately processed to fit the needs of our neural network.\n",
    "2. **Feature Engineering**: We enhanced our dataset with embeddings and engineered features that would provide our model with the depth of data required for accurate predictions.\n",
    "3. **Model Building**: The neural network was designed with modern architectures involving transformers and attention mechanisms to handle both regression and classification tasks efficiently.\n",
    "4. **Hyperparameter Tuning**: Optuna was utilized to find the best hyperparameters, ensuring our model performed optimally under various configurations.\n",
    "5. **Model Training and Evaluation**: We trained our model using the optimized parameters and evaluated its performance using a structured test dataset to assess both its predictive accuracy and classification capabilities.\n",
    "6. **Logging and Checkpointing**: Throughout the process, we employed advanced logging and checkpointing strategies to monitor our experiments and save the best models for further analysis.\n",
    "\n",
    "As we conclude the technical aspects of our project, the next essential step is to compile all findings, insights, and performance metrics into a comprehensive report. This report will analyze the model's effectiveness, discuss any challenges encountered, and provide a detailed review of the machine learning strategies employed. The insights gained from this report will not only validate our approach but also guide future projects in enhancing model accuracy and efficiency.\n",
    "\n",
    "By documenting every step and reflecting on our project's outcomes, we can ensure that the knowledge gained contributes to the broader field of AI and machine learning, helping to inform best practices and inspire future innovations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
